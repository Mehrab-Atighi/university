---
title: "ISLR"
output:
  word_document: default
  html_document: default
---



# 2.3 Introduction to R 

## 2.3.1 Basic Commands
```{r}
x <- c(1,3,2,5)
x
```

```{r}
x <- c(1,6,2)
x
y <- c(1,4,3)
```

```{r}
length(x)
length(y)
x+y
```

```{r}
ls()
```

```{r}
rm(list=ls())
```

```{r}
x <- matrix(data=c(1,2,3,4),nrow=2,ncol=2)
x
```

```{r}
x <- matrix(c(1,2,3,4),2,2)
x
```

```{r}
matrix(c(1,2,3,4),2,2,byrow=TRUE)
```

```{r}
sqrt(x)
```

```{r}
x^2
```

```{r}
x <- rnorm(50)
y <- x + rnorm(50,mean=50,sd=0.1)
cor(x,y)
```

```{r}
set.seed(1303)
rnorm(50)
```

```{r}
set.seed(3)
y <- rnorm(100)
mean(y)
var(y)
sqrt(var(y))
sd(y)
```


## 2.3.2 Graphics 
```{r}
x <- rnorm(100)
y <- rnorm(100)
plot(x,y)
plot(x,y,xlab="this is the x-axis",ylab="this is y-axis",main="Plot of X vs Y")
```

```{r}
pdf("Figure.pdf")
plot(x,y,col="green")
dev.off()
```

```{r}
x <- seq(1,10)
x <- seq(-pi,pi,length=50)
```

```{r}
y <- x
f <- outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45,add=T)
fa <- (f-t(f))/2
contour(x,y,fa,nlevels=15)
```

```{r}
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
persp(x,y,fa,theta=30,phi=20)
persp(x,y,fa,theta=30,phi=70)
persp(x,y,fa,theta=30,phi=40)
```


## 2.3.3 Indexing data 
```{r}
A <- matrix(1:16,4,4)
A
```

```{r}
A[2,3]
```

```{r}
A[c(1,3),c(2,4)]
```

```{r}
A[1:3,2:4]
```

```{r}
A[1:2,]
```

```{r}
A[,1:2]
```

```{r}
A[-c(1,3),]
```

```{r}
A[-c(1,3),-c(1,3,4)]
```

```{r}
dim(A)
```

## 2.3.4 Loading data
```{r}
# Auto <- read.table("Auto.data")
# Auto <- read.csv("Auto.csv",header=T,na.strings="?")
# Auto <- na.omit(Auto)
# fix(Auto)
# names(Auto)
```

## 2.3.5 Additional graphic and numerical summaries
```{r}
plot(ISLR::Auto$cylinders,ISLR::Auto$mpg)
attach(ISLR::Auto)
plot(cylinders,mpg)
cylinders = as.factor(cylinders)
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders",ylab="MPG")
hist(mpg)
hist(mpg,col=2)
hist(mpg,col=2,breaks=15)
pairs(ISLR::Auto)
pairs(~mpg + displacement + horsepower + weight + acceleration, ISLR::Auto)
plot(horsepower,mpg)
identify(horsepower, mpg, name)
summary(ISLR::Auto)
summary(mpg)
```



# 3.6 Linear Regression 

## 3.6.1 Libraries 
```{r}
#install.packages("MASS")
#install.packages("ISLR")
library(MASS)
library(ISLR)
```

## 3.6.2 Simple linear regression 
```{r}
names(Boston)
```

```{r}
lm.fit <- lm(medv ~ lstat,data=Boston)
lm.fit
summary(lm.fit)
```

```{r}
names(lm.fit)
```

```{r}
coef(lm.fit)
confint(lm.fit)
```

```{r}
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval="confidence")
```

```{r}
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval="prediction")
```

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

```{r}
plot(predict(lm.fit),residuals(lm.fit))
```

```{r}
plot(predict(lm.fit),rstudent(lm.fit))
```

```{r}
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```


## 3.6.3 Multiple Linear Regression 
```{r}
library(MASS)
library(ISLR)
```
lm.fit <- lm(medv~lstat+age,data=Boston)
summary(lm.fit)
```

```{r}
lm.fit <- lm(medv~.,data=Boston)
summary(lm.fit)
```

```{r}
#install.packages("car")
library(car)
vif(lm.fit)
```

```{r}
lm.fit1 <- lm(medv~.-age,data=Boston)
summary(lm.fit1)
```

```{r}
lm.fit1 <- update(lm.fit,~.-age)
```

## 3.6.4 Interaction Terms 
```{r}
summary(lm(medv~lstat*age,data=Boston))
```

## 3.6.5 Non-linear transformations of the predictors 
```{r}
lm.fit2 <- lm(medv~lstat+I(lstat^2),Boston)
summary(lm.fit2)
```

```{r}
lm.fit <- lm(medv ~ lstat,Boston)
anova(lm.fit,lm.fit2)
```

```{r}
par(mfrow=c(2,2))
plot(lm.fit2)
```

```{r}
lm.fit5 <- lm(medv~poly(lstat,5),Boston)
summary(lm.fit5)
```

```{r}
summary(lm(medv~log(rm),data=Boston))
```

## 3.6.6 Qualitative predictors 
```{r}
names(Carseats)
```

```{r}
lm.fit <- lm(Sales~.+Income:Advertising+Price:Age,Carseats)
summary(lm.fit)
```

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

## 3.6.7 Writing functions 
```{r}
LoadLibaries <- function() {
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibaries()
```



# 4.6 Logistic Regression, LDA, QDA, and KNN

## 4.6.1 The stock market data 

```{r}
library(ISLR)
names(Smarket)
dim(Smarket)
summary(Smarket)
```

```{r}
cor(Smarket[,-9])
```

```{r}
attach(Smarket)
plot(Volume)
```


## 4.6.2 Logistic regression 
```{r}
glm.fit <- glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data=Smarket, family=binomial)
summary(glm.fit)
```

```{r}
coef(glm.fit)
summary(glm.fit)$coef
```

```{r}
glm.probs <- predict(glm.fit, type="response")
glm.probs[1:10]
contrasts(Direction)
```

```{r}
glm.pred <- rep("Down",1250)
glm.pred[glm.probs>0.5]="Up"
table(glm.pred,Direction)
mean(glm.pred==Direction)
```

```{r}
train <- (Year<2005)
Smarket.2005 <- Smarket[!train,]
dim(Smarket.2005)
Direction.2005 <- Direction[!train]
```

```{r}
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train)
glm.probs <- predict(glm.fit,Smarket,subset=train)
```

```{r}
glm.pred <- rep("Down",252)
glm.pred[glm.probs>0.5]="Up"
# table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)
```

```{r}
glm.fit <- glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)
glm.probs <- predict(glm.fit,Smarket.2005,type="response")
glm.pred <- rep("Down",252)
glm.pred[glm.probs>0.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
```

```{r}
predict(glm.fit, newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
```


## 4.6.3 Linear Discriminant Analysis 
```{r}
library(MASS)
lda.fit <- lda(Direction~Lag1 + Lag2, data=Smarket,subset=train)
lda.fit
```

```{r,fig.height=10}
plot(lda.fit)
```

```{r}
lda.pred <- predict(lda.fit,Smarket.2005)
names(lda.pred)
```

```{r}
lda.class <- lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
```

```{r}
sum(lda.pred$posterior[,1]>=0.5)
sum(lda.pred$posterior[,1]<=0.5)
lda.pred$posterior[1:20,1]
lda.class[1:20]
```

```{r}
sum(lda.pred$posterior[,1]>0.9)
```

## 4.6.4 Quadratic discriminant analysis 
```{r}
qda.fit <- qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
```

```{r}
qda.class <- predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
```


## 4.6.5 K-nearest neighbors 
```{r}
#install.packages("class")
library(class)
train.X <- cbind(Lag1,Lag2)[train,]
test.X <- cbind(Lag1,Lag2)[!train,]
train.Direction <- Direction[train]
```

```{r}
set.seed(1)
knn.pred <- knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
```

```{r}
knn.pred <- knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
```


## 4.6.6 An application to caravan insurance data 

```{r,warning=F}
dim(Caravan)
attach(Caravan)
summary(Purchase)
348/5822
```

```{r}
standardized.X <- scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
```

```{r}
test <- 1:1000
train.X <- standardized.X[-test,]
test.X <- standardized.X[test,]
train.Y <- Purchase[-test]
test.Y <- Purchase[test]
set.seed(1)
knn.pred <- knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
```

```{r}
table(knn.pred,test.Y)
9/(68+9)
```

```{r}
knn.pred <- knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
5/26
```

```{r}
knn.pred <- knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
```

```{r}
glm.fit <- glm(Purchase~.,data=Caravan,family=binomial,subset=-test)
glm.probs <- predict(glm.fit,Caravan[test,],type="response")
glm.pred <- rep("No",1000)
glm.pred[glm.probs>0.5]<-"Yes"
table(glm.pred,test.Y)
```

```{r}
glm.pred <- rep("No",1000)
glm.pred[glm.probs>0.25]<-"Yes"
table(glm.pred,test.Y)
11/(22+11)
```




# 5.3 Cross-Validation and the Bootstrap 

## 5.3.1 The validation set approach 

```{r}
library(ISLR)
set.seed(1)
train <- sample(392,196)
```

```{r}
lm.fit <- lm(mpg~horsepower,data=ISLR::Auto,subset=train)
```

```{r}
attach(ISLR::Auto)
mean((mpg-predict(lm.fit,ISLR::Auto))[-train]^2)
```

```{r}
lm.fit2 <- lm(mpg~poly(horsepower,2),data=ISLR::Auto,subset=train)
mean((mpg-predict(lm.fit2,ISLR::Auto))[-train]^2)
```

```{r}
lm.fit3 <- lm(mpg~poly(horsepower,3),data=ISLR::Auto,subset=train)
mean((mpg-predict(lm.fit3,ISLR::Auto))[-train]^2)
```

```{r}
set.seed(2)
train=sample(392,196)
lm.fit <- lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,ISLR::Auto))[-train]^2)
```

```{r}
lm.fit2 <- lm(mpg~poly(horsepower,2),data=ISLR::Auto,subset=train)
mean((mpg-predict(lm.fit2,ISLR::Auto))[-train]^2)
```

```{r}
lm.fit3 <- lm(mpg~poly(horsepower,3),data=ISLR::Auto,subset=train)
mean((mpg-predict(lm.fit3,ISLR::Auto))[-train]^2)
```


## 5.3.2 Leave one out cross validation 
```{r}
glm.fit <- glm(mpg~horsepower,data=ISLR::Auto)
coef(glm.fit)
```

```{r}
lm.fit <- lm(mpg~horsepower,data=ISLR::Auto)
coef(lm.fit)
```

```{r}
#install.packages("boot")
library(boot)
glm.fit <- glm(mpg~horsepower,data=ISLR::Auto)
cv.err <- cv.glm(ISLR::Auto,glm.fit)
cv.err$delta
```

```{r}
names(cv.err)
```

```{r}
cv.error <- rep(0,5)
for(i in 1:5){
  glm.fit <- glm(mpg~poly(horsepower,i),data=ISLR::Auto)
  cv.error[i] <- cv.glm(ISLR::Auto,glm.fit)$delta[1]
}
cv.error
plot(cv.error)
```


## 5.3.3 k-fold cross validation
```{r}
set.seed(17)
cv.error.10 <- rep(0,10)
for(i in 1:10){
  glm.fit <- glm(mpg~poly(horsepower,i),data=ISLR::Auto)
  cv.error.10[i] <- cv.glm(ISLR::Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
plot(cv.error)
```


## 5.3.4 The bootstrap 
```{r}
alpha.fn <- function(data,index){
  X <- data$X[index]
  Y <- data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)))
}
```

```{r}
alpha.fn(Portfolio,1:100)
```

```{r}
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
```

```{r}
boot(Portfolio,alpha.fn,R=1000)
```

## Estimating the accuracy of a linear regression model 
The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method 

```{r}
boot.fn <- function(data,index) {
  return(coef(lm(mpg~horsepower,data=data,subset=index)))
}
boot.fn(ISLR::Auto,1:392)
```

```{r}
set.seed(1)
boot.fn(ISLR::Auto,sample(392,392,replace=T))
boot.fn(ISLR::Auto,sample(392,392,replace=T))
```

```{r}
boot(ISLR::Auto,boot.fn,1000)
```

```{r}
summary(lm(mpg~horsepower,data=ISLR::Auto))$coef
```

```{r}
boot.fn <- function(data,index){
  coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
}
set.seed(1)
boot(ISLR::Auto,boot.fn,1000)
```


```{r}
summary(lm(mpg~horsepower+I(horsepower^2),data=ISLR::Auto))$coef
```




# 6.5 Lab1: Subset Selection Methods 

## 6.5.1 Best subset selection 
```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
```

```{r}
sum(is.na(Hitters$Salary))
```

```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
#install.packages("leaps")
library(leaps)
regfit.full <- regsubsets(Salary~.,Hitters)
summary(regfit.full)
```

```{r}
regfit.full <- regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary <- summary(regfit.full)
names(reg.summary)
```

```{r}
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of variables",ylab="Adjusted Rsq",type="l")
points(11,reg.summary$adjr2[11],col="red",pch=20)
```

```{r}
which.max(reg.summary$adjr2)
```

```{r}
plot(reg.summary$cp,xlab="Number of variables",ylab="Cp",type="l")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of variables",ylab="BIC",type="l")
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
```

```{r,fig.height=8}
plot(regfit.full,scale="r2")
```

```{r,fig.height=8}
plot(regfit.full,scale="adjr2")
```

```{r,fig.height=8}
plot(regfit.full,scale="Cp")
```

```{r,fig.height=8}
plot(regfit.full,scale="bic")
```


## 6.5.2 Forward and backward stepwise selection 
```{r}
regfit.fwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
```

```{r}
regfit.bwd <- regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)
```

```{r,fig.height=8}
plot(regfit.fwd)
```

```{r}
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
```


## 6.5.3 Choosing among models using the validation set approach and cross validation 

```{r}
set.seed(1)
train=sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
test=(!train)
```

```{r}
regfit.best <- regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat <- model.matrix(Salary~.,data=Hitters[test,])
```

```{r}
val.errors <- rep(NA,19)
for(i in 1:19){
  coefi <- coef(regfit.best,id=i)
  pred <- test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
```

```{r}
val.errors
```

```{r}
which.min(val.errors)
coef(regfit.best,10)
```

```{r}
predict.regsubsets <- function(object,newdata,id,...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object,id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}
```

```{r}
regfit.best <- regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(regfit.best,10)
```

```{r}
k <- 10 
set.seed(1)
folds <- sample(1:k,nrow(Hitters),replace=T)
cv.errors <- matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))
```

```{r}
for (j in 1:k){
  best.fit <- regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred <- predict(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean((Hitters$Salary[folds==j]-pred)^2)
  }
}
```

```{r}
mean.cv.errors <- apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type="b")
```

```{r}
reg.best <- regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(reg.best,11)
```


# 6.6 Lab2: Ridge Regression and the Lasso 

## 6.6.1 Ridge regression 

```{r}
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
```

```{r}
#install.packages("glmnet")
library(glmnet)
grid <- 10^seq(10, -2, length=100)
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)
```

```{r}
plot(ridge.mod)
```

```{r}
dim(coef(ridge.mod))
```

```{r}
ridge.mod$lambda[50]
```

```{r}
coef(ridge.mod)[,50]
```

```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

```{r}
predict(ridge.mod, s=50, type="coefficients")[1:20,]
```

```{r}
set.seed(1)
train <- sample(1:nrow(x),nrow(x)/2)
test <- (-train)
y.test <- y[test]
```

```{r}
ridge.mod <- glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred <- predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
plot(ridge.mod)
```

```{r}
plot(ridge.pred)
```

```{r}
mean((mean(y[train])-y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
ridge.pred <- predict(ridge.mod, s=0, newx=x[test,])
mean((ridge.pred - y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod, s=0, type="coefficients")[1:20,]
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
```

```{r}
ridge.pred <- predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
out <- glmnet(x,y,alpha=0)
predict(out, type="coefficients",s=bestlam)[1:20,]
```


## 6.6.2 The Lasso 

```{r}
lasso.mod <- glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred-y.test)^2)
```

```{r}
out <- glmnet(x,y,alpha=1,lambda=grid)
lasso.coef <- predict(out, type="coefficients",s=bestlam)[1:20,]
lasso.coef
```



# 6.7 Lab3: PCR and PLS Regression 

## 6.7.1 Principal Components Regression 

```{r}
#install.packages("pls")
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary~., data=Hitters, scale=T, validation="CV")
summary(pcr.fit)
```

```{r}
plot(pcr.fit$projection)
```

```{r}
validationplot(pcr.fit, val.type="MSEP")
```

```{r}
set.seed(1)
pcr.fit <- pcr(Salary~., data=Hitters, subset=train, scale=T, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
```

```{r}
pcr.pred <- predict(pcr.fit, x[test,],ncomp=7)
mean((pcr.pred - y.test)^2)
```

```{r}
pcr.fit <- pcr(y~x, scale=T, ncomp=7)
summary(pcr.fit)
```


## 6.7.2 Partial Least Squares 
```{r}
set.seed(1)
pls.fit <- plsr(Salary~., data=Hitters, subset=train, scale=T, validation="CV")
summary(pls.fit)
plot(pls.fit)
validationplot(pls.fit, val.type="MSEP")
```

```{r}
pls.pred <- predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
```

```{r}
pls.fit <- plsr(Salary~., data=Hitters, scale=T, ncomp=2)
summary(pls.fit)
```




# 7.8 Lab: Non-Linear Modeling 

## 7.8.1 Polynomial Regression and Step Functions 

```{r}
library(ISLR)
attach(Wage)
```

```{r}
fit <- lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
```

```{r}
fit2 <- lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
```

```{r}
fit2a <- lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)
```

```{r}
fit2b <- lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b)
```

```{r}
agelims <- range(age)
age.grid <- seq(from=agelims[1],to=agelims[2])
preds <- predict(fit, newdata=list(age=age.grid),se=T)
se.bands <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
```

```{r}
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age, wage, xlim=agelims,cex=0.5,col="darkgrey")
title("Degree-4 Polynomial", outer=T)
lines(age.grid, preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
```


```{r}
preds2 <- predict(fit2, newdata=list(age=age.grid),se=T)
max(abs(preds$fit-preds2$fit))
```

```{r}
fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~poly(age,2),data=Wage)
fit.3 <- lm(wage~poly(age,3),data=Wage)
fit.4 <- lm(wage~poly(age,4),data=Wage)
fit.5 <- lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
```

```{r}
coef(summary(fit.5))
```

```{r}
(-11.983)^2
```

```{r}
fit.1 <- lm(wage~education+age,data=Wage)
fit.2 <- lm(wage~education+poly(age,2),data=Wage)
fit.3 <- lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
```

```{r}
fit <- glm(I(wage>250)~poly(age,4),data=Wage,family=binomial)
summary(fit)
```

```{r}
preds <- predict(fit, newdata=list(age=age.grid),se=T)
```

```{r}
pfit <- exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit <- cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
se.bands <- exp(se.bands.logit)/(1+exp(se.bands.logit))
```

```{r}
preds <- predict(fit, newdata=list(age=age.grid),type="response",se=T)
```

```{r}
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,0.2))
points(jitter(age),I((wage>250)/5),cex=0.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
```

```{r}
table(cut(age,4))
```

```{r}
fit <- lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
```


## 7.8.2 Splines 

```{r}
#install.packages("splines")
library(splines)
fit <- lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred <- predict(fit, newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")
```

```{r}
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
```

```{r}
fit2 <- lm(wage~ns(age,df=4),data=Wage)
pred2 <- predict(fit2, newdata=list(age=age.grid),se=T)
plot(age,wage)
lines(age.grid, pred2$fit,col="red",lwd=2)
```

```{r}
plot(age,wage,xlim=agelims,cex=0.5,col="darkgrey")
title("Smoothing Spline")
fit <- smooth.spline(age,wage,df=16)
fit2 <- smooth.spline(age,wage,cv=T)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF", "6.8 DF"), col=c("red","blue"),lty=1,lwd=2,cex=0.8)
```

```{r}
plot(age,wage,xlim=agelims,cex=0.5,col="darkgrey")
title("Local Regression")
fit <- loess(wage~age, span=0.2, data=Wage)
fit2 <- loess(wage~age, span=0.5, data=Wage)
lines(age.grid, predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid, predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=0.8)
```


## 7.8.3 GAMs 

```{r}
gam1 <- lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
```

```{r}
#install.packages("gam")
library(gam)
gam.m3 <- gam(wage~s(year,4)+s(age,5)+education,data=Wage)
```

```{r}
par(mfrow=c(1,3))
plot(gam.m3,se=T,col="blue")
```

```{r}
plot.Gam(gam1,se=T,col="red")
```

```{r}
gam.m1 <- gam(wage~s(age,5)+education,data=Wage)
gam.m2 <- gam(wage~year+s(age,5)+education,data=Wage)
gam.m3 <- gam(wage~s(year,4)+s(age,5)+education,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")
```

```{r}
summary(gam.m3)
```

```{r}
preds <- predict(gam.m3,newdata=Wage)
```

```{r}
gam.lo <- gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot.Gam(gam.lo,se=T,col="green")
```

```{r}
gam.lo.i <- gam(wage~lo(year,age,span=0.5)+education,data=Wage)
```

```{r}
#install.packages("akima")
library(akima)
plot(gam.lo.i)
```

```{r}
gam.lr <- gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
```

```{r}
table(education,I(wage>250))
```

```{r}
gam.lr.s <- gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1.< HS Grad"))
plot(gam.lr.s,se=T,col="green")
```




# 8.3 Lab: Decision Trees

## Fitting Classification Trees

```{r}
#install.packages("tree")
library(tree)
library(ISLR)
attach(Carseats)
High <- ifelse(Sales<=8, "No", "Yes")
```

```{r}
Carseats <- data.frame(Carseats, High)
```

```{r}
#tree.carseats <- tree(High~.-Sales,Carseats)
#summary(tree.carseats)
```

```{r,fig.width=10,fig.height=10}
#plot(tree.carseats)
#text(tree.carseats,pretty=0)
```

```{r}
#tree.carseats
```

```{r}
#set.seed(2)
#train <- sample(1:nrow(Carseats),200)
#Carseats.test <- Carseats[-train,]
#High.test <- High[-train]
#tree.carseats <- tree(High~CompPrice+Income+Advertising+Population+Price+ShelveLoc+Age+Education+Urban+US, Carseats, subset=train)
#tree.pred <- predict(tree.carseats, Carseats.test, type="class")
#table(tree.pred, High.test)
#(86+57)/200
```

```{r}
#set.seed(3)
#cv.carseats <- cv.tree(tree.carseats,FUN=prune.misclass)
#names(cv.carseats)
```

```{r}
#cv.carseats$size
#cv.carseats$dev
#cv.carseats$k
#cv.carseats$method
```

```{r}
#par(mfrow=c(1,2))
#plot(cv.carseats$size,cv.carseats$dev,type="b")
#plot(cv.carseats$k,cv.carseats$dev,type="b")
```

```{r,fig.width=8,fig.height=8}
#prune.carseats <- prune.misclass(tree.carseats,best=9)
#plot(prune.carseats)
#text(prune.carseats,pretty=0)
```

```{r}
#tree.pred <- predict(prune.carseats, Carseats.test, type="class")
#table(tree.pred, High.test)
#(94+60)/200
```

```{r,fig.height=8,fig.width=8}
#prune.carseats <- prune.misclass(tree.carseats,best=15)
#plot(prune.carseats)
#text(prune.carseats,pretty=0)
#tree.pred <- predict(prune.carseats, Carseats.test, type="class")
#table(tree.pred, High.test)
#(86+62)/200
```



## 8.3.2 Fitting Regression Trees

```{r}
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston <- tree(medv~., Boston, subset=train)
summary(tree.boston)
```


```{r,fig.width=6,fig.height=6}
plot(tree.boston)
text(tree.boston, pretty=0)
```

```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type="b")
```

```{r}
prune.boston <- prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
```

```{r}
yhat <- predict(tree.boston, newdata=Boston[-train,])
boston.test <- Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```



## 8.3.3 Bagging and Random Forest 

```{r}
#install.packages("randomForest")
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, importance=T)
bag.boston
```

```{r}
yhat.bag <- predict(bag.boston,newdata <- Boston[-train,])
plot(yhat.bag,boston.test)
abline(0,1,col="red")
mean((yhat.bag-boston.test)^2)
```

```{r}
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, ntree=25)
yhat.bag <- predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

```{r}
set.seed(1)
rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry=6, importance=T)
yhat.rf <- predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

```{r}
importance(rf.boston)
```

```{r,fig.width=8,fig.height=6}
varImpPlot(rf.boston)
```


## 8.3.4 Boosting 

```{r}
#install.packages("gbm")
library(gbm)
set.seed(1)
boost.boston <- gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000, interaction.depth=4)
summary(boost.boston)
```

```{r}
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```


```{r}
yhat.boost <- predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

```{r}
boost.boston <- gbm(medv~., data=Boston[train,],distribution="gaussian",n.trees=5000, interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost <- predict(boost.boston, newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
```





# 9.6 Lab: Support Vector Machines 

## 9.6.1 Support Vector Classifier 

```{r}
#install.packages("e1071")
library(e1071)
```

```{r}
set.seed(1)
x <- matrix(rnorm(20*2),ncol=2)
y <- c(rep(-1,10),rep(1,10))
x[y==1,] <- x[y==1,] + 1
```

```{r}
plot(x,col=(3-y))
```

```{r}
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel="linear",cost=10, scale=F)
plot(svmfit,dat)
```

```{r}
svmfit$index
```

```{r}
summary(svmfit)
```

```{r}
svmfit <- svm(y~., data=dat, kernel="linear",cost=0.1,scale=F)
plot(svmfit,dat)
```

```{r}
svmfit$index
```

```{r}
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

```{r}
summary(tune.out)
```

```{r}
plot(tune.out)
```

```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

```{r}
xtest <- matrix(rnorm(20*2),ncol=2)
ytest <- sample(c(-1, 1),20, rep=T)
xtest[ytest==1,] <- xtest[ytest==1,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))
```

```{r}
ypred <- predict(bestmod, testdat)
table(predict=ypred, truth=testdat$y)
```

```{r}
svmfit <- svm(y~., data=dat, kernel="linear", cost=0.01, scale=F)
ypred <- predict(svmfit, testdat)
table(predict=ypred, truth=testdat$y)
```

```{r}
x[y==1,] <- x[y==1,] + 0.5
plot(x,col=(y+5)/2,pch=19)
```

```{r}
dat <- data.frame(x=x, y=as.factor(y))
svmfit <- svm(y~., data=dat, kernel="linear",cost=1e5)
summary(svmfit)
```

```{r}
plot(svmfit,dat)
```

```{r}
svmfit <- svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,dat)
```


## 9.6.2 Support vector machine

```{r}
set.seed(1)
x <- matrix(rnorm(200*2),ncol=2)
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] - 2
y <- c(rep(1,150),rep(2,50))
dat <- data.frame(x=x, y=as.factor(y))
plot(x,col=y)
```

```{r}
train <- sample(200,100)
svmfit <- svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
plot(svmfit, dat[train,])
```

```{r}
summary(svmfit)
```

```{r}
svmfit <- svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1e5)
plot(svmfit, dat[train,])
```

```{r}
set.seed(1)
tune.out <- tune(svm, y~., data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
```

```{r}
table(true=dat[-train,"y"],pred=predict(tune.out$best.model,newx=dat[-train,]))
```


## 9.6.3 ROC Curves

```{r}
#install.packages("ROCR")
library(ROCR)
rocplot <- function(pred, truth, ...){
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
```


```{r}
svmfit.opt <- svm(y~., data=dat[train,],kernel="radial",gamma=2, cost=1, decision.values=T)
fitted <- attributes(predict(svmfit.opt,dat[train,],decision.values=T))$decision.values
```

```{r,fig.width=8,fig.height=4}
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
```

```{r,fig.width=8,fig.height=4}
svmfit.flex <- svm(y~., data=dat[train,],kernel="radial",gamma=50, cost=1, decision.values=T)
fitted <- attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
rocplot(fitted,dat[train,"y"],col="red")
```


```{r}
fitted <- attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")
fitted <- attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")
```



## 9.6.4 SVM With Multiple Classes 
```{r}
set.seed(1)
x <- rbind(x,matrix(rnorm(50*2),ncol=2))
y <- c(y,rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat<-data.frame(x=x,y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=(y+1))
```

```{r}
svmfit <- svm(y~., data=dat, kernel="radial",cost=10,gamma=1)
plot(svmfit,dat)
```


## 9.6.5 Application to Gene Expression Data
```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
```

```{r}
table(Khan$ytrain)
table(Khan$ytest)
```

```{r}
dat <- data.frame(x=Khan$xtrain,y=as.factor(Khan$ytrain))
out <- svm(y~., data=dat, kernel="linear", cost=10)
summary(out)
table(out$fitted, dat$y)
```

```{r}
dat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te <- predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
```




# 10.4 Lab1: Principal Components Analysis 

```{r}
states <- row.names(USArrests)
states
```

```{r}
names(USArrests)
```

```{r}
apply(USArrests,2,mean)
```

```{r}
apply(USArrests,2,var)
```

```{r}
pr.out <- prcomp(USArrests,scale=T)
names(pr.out)
```

```{r}
pr.out$center
pr.out$scale
```

```{r}
pr.out$rotation
```

```{r}
plot(pr.out)
```

```{r}
dim(pr.out$x)
```

```{r,fig.width=8,fig.height=8}
biplot(pr.out,scale=0)
```

```{r,fig.width=8,fig.height=8}
pr.out$rotation <- -pr.out$rotation
pr.out$x <- -pr.out$x
biplot(pr.out,scale=0)
```

```{r}
pr.out$sdev
pr.var <- pr.out$sdev^2
pr.var
```

```{r}
pve <- pr.var/sum(pr.var)
pve
```


```{r}
plot(pve, xlab="Principal Component",ylab="Proportion of Variance Explained",ylim=c(0,1),type="b")
```

```{r}
plot(cumsum(pve),xlab="Principal Component",ylab="Cummulative Proportion of Variance Explained",ylim=c(0,1),type="b")
```

```{r}
a <- c(1,2,8,-3)
cumsum(a)
```




# 10.5 Lab2: Clustering 

## 10.5.1 K-Means Clustering 

```{r}
set.seed(2)
x <- matrix(rnorm(50*2),ncol=2)
x[1:25,1] <- x[1:25,1] + 3
x[1:25,2] <- x[1:25,2] - 4
plot(x)
```

```{r}
km.out <- kmeans(x,2,nstart=20)
names(km.out)
```

```{r}
km.out$cluster
```

```{r}
plot(x,col=(km.out$cluster+1),main="K-Means Clustering Results with K=2",xlab="",ylab="",pch=20,cex=2)
```

```{r}
set.seed(4)
km.out <- kmeans(x,3,nstart=20)
km.out
```

```{r}
plot(x,col=(km.out$cluster+1),main="K-Means Clustering Results with K=3",xlab="",ylab="",pch=20, cex=2)
```

```{r}
set.seed(3)
km.out <- kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out <- kmeans(x,3,nstart=20)
km.out$tot.withinss
```



## 10.5.2 Hierarchical Clustering 

```{r}
hc.complete <- hclust(dist(x),method="complete")
```

```{r}
hc.average <- hclust(dist(x),method="average")
hc.single <- hclust(dist(x),method="single")
```

```{r,fig.height=8,fig.width=8}
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage",xlab="",sub="",cex=.9)
plot(hc.average,main="Average Linkage",xlab="",sub="",cex=.9)
plot(hc.single,main="Single Linkage", xlab="",sub="",cex=.9)
```

```{r}
cutree(hc.complete,2)
cutree(hc.average,2)
cutree(hc.single,2)
```

```{r}
cutree(hc.single,4)
```

```{r,fig.height=6}
xsc <- scale(x,center=F,scale=T)
plot(hclust(dist(xsc),method="complete"),main="Hierarchical Clustering with Scaled Observations")
```

```{r,fig.height=6}
xsc <- scale(x)
plot(hclust(dist(xsc),method="complete"),main="Hierarchical Clustering with Scaled Observations")
```

```{r,fig.width=6}
x <- matrix(rnorm(30*3),ncol=3)
dd <- as.dist(1-cor(t(x)))
plot(hclust(dd,method="complete"),main="Complete Linkage with Correlation-Based Distance",xlab="",sub="")
```



# 10.6 Lab3: NCI60 Data Example 

```{r}
library(ISLR)
nci.labs <- NCI60$labs
nci.data <- NCI60$data
dim(nci.data)
```

```{r}
nci.labs[1:4]
table(nci.labs)
```


## 10.6.1 PCA on the NCI60 Data 

```{r}
pr.out <- prcomp(nci.data,scale=TRUE)
```

```{r}
Cols <- function(vec){
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
```

```{r,fig.height=6,fig.width=8}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2],col=Cols(nci.labs),pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)],col=Cols(nci.labs),pch=19,xlab="Z1",ylab="Z3")
```

```{r}
summary(pr.out)
```

```{r}
plot(pr.out)
```

```{r,fig.width=6,fig.height=4}
pve <- 100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve, type="o",ylab="PVE",xlab="Principal Component",col="blue")
plot(cumsum(pve),type="o",ylab="Cummulative PVE",xlab="Principal Component",col="brown3")
```


## 10.6.2 Clustering the Observations of the NCI60 Data

```{r,fig.width=10,fig.height=6}
sd.data <- scale(nci.data, FALSE, TRUE)
par(mfrow=c(1,3))
data.dist <- dist(sd.data)
plot(hclust(data.dist),labels=nci.labs,main="Complete Linkage",xlab="",sub="",ylab="")
plot(hclust(data.dist,method="average"),labels=nci.labs,main="Average Linkage",xlab="",sub="",ylab="")
plot(hclust(data.dist,method="single"),labels=nci.labs,main="Single Linkage",xlab="",sub="",ylab="")
```

```{r}
hc.out <- hclust(dist(sd.data))
hc.clusters <- cutree(hc.out,4)
table(hc.clusters,nci.labs)
```

```{r,fig.width=8,fig.height=8}
par(mfrow=c(1,1))
plot(hc.out,labels=nci.labs)
abline(h=139,col="red")
```

```{r}
hc.out
```

```{r}
set.seed(2)
km.out <- kmeans(sd.data,4,nstart=20)
km.clusters <- km.out$cluster
table(km.clusters,hc.clusters)
```

```{r,fig.width=8,fig.height=6}
hc.out <- hclust(dist(pr.out$x[,1:5]))
plot(hc.out,labels=nci.labs,main="Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out,4),nci.labs)
```