---
title: "Exercise chapter thiry"
author: "Mehrab Atighi"
date: "4/4/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
solve 5 bottom questions:

# 1)  
## a) According to the Bottem data make main model woth 18 points and get summary function?


```{r}
#Exercise one:
rm(list=ls())
#intruducing Data:
x<-c(1.5,1.7,2,2.2,2.5,2.5,2.7,2.9,3,3.5,3.8,4.2,4.3,4.6,4,5.1,5.2,5.5)
y<-c(1,2.5,3.5,3,3.1,3.6,2.2,3.9,4,4,4.2,4.1,4.8,1.2,5.1,5.1,4.8,5.3)

A=3.4;B=9.5;C=9.5
a=8;b=8;c=2.5
#Make Models and get needed information:
#Make First Regression Model(with main Data)
#a)
fit1<-lm(y~x)
#Get needed information from this Model
summary(fit1)
```

now we can see that or p_value for x is lower than 0.05 and the betha0 = 1.4616,

betha1 = 0.6387,  the Residual standard error = 1.025,

R-squared = 0.3902,t(betha1) = 3.200


## b) According to the Bottem data make main model woth 18 points+A&a and get summary function?


```{r}
#b)
#make Second regssion Model(With A point & main Data)
x[19]= A ; y[19]= a
fit2<-lm(y~x)
#Get needed information from this Model
summary(fit2)
```
now we can see that or p_value for x is lower than 0.05 and the betha0 = 1.6914,

betha1 = 0.6387, the Residual standard error = 1.432,

R-squared = 0.2358,t(betha1) = 2.290

## c) According to the Bottem data make main model woth 18 points+B&b and get summary function?


```{r}
#c)
#make third regssion Model(With B point & main Data)
x[19]= B ; y[19]= b
fit3<-lm(y~x)
#Get needed information from this Model
summary(fit3)
```
now we can see that or p_value for x is lower than 0.05 and the betha0 = 1.3223,

betha1 = 0.6828,    the Residual standard error = 0.9973,

R-squared = 0.6296,t(betha1)= 5.375

## d) According to the Bottem data make main model woth 18 points+C&c and get summary function?

```{r}
#d)
#make forth regssion Model(With C point & main Data)
x[19]= C ; y[19]= c
fit4<-lm(y~x)
#Get needed information from this Model
summary(fit4)
```
now we can see that or p_value for x is not lower than 0.05 and the betha0 = 2.9518,

betha1 = 0.1671, the Residual standard error = 1.262,

R-squared = 0.05978,t(betha1)=1.040 

End.

-----------------------------------------------------------------------

# 2) show that the bottem datas variables (x1,x2) have linear relation.
```{r}
#Exercise Two:
x1<-c(2,8,6,10)
x2<-c(6,9,8,10)
#produce variables plot:
par(mfrow=c(1,1))
plot(x1,x2,col="orange")

```

We can see a possetive relation between X1 & X2,
so we can say when they have a linear relation.
it will be possible to make a mistake make a regression model.
End.

-----------------------------------------------------------------------

# 3) This question involves the use of simple linear regression on the Auto data set.
## a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example:
```{r}
#Exercise Third(eigth of Book):
#library Data:
install.packages("ISLR")
library("ISLR")
y<-Auto$mpg
x<-Auto$horsepower
#a)
fit1<-lm(y~x)
summary(fit1)
```

### i. Is there a relationship between the predictor and the response?

```{r}
#i)
plot(x, y, col = "orange", type = "p")

```

We can see a nagative relation between X & Y its fixed for x > 150 .

### ii. How strong is the relationship between the predictor and the response?

```{r}
#ii
summary(fit1)[8]
anova(fit1)[5]
```

the r.squared is equal to 0.6059483.

the p value of the Anova of our model is very lower than 0.05.

So we can say that this linear regression model is significant and the r. squared show that its not very strong relation!.

### iii. Is the relationship between the predictor and the response positive or negative?

```{r}
#iii)
cor(x,y)
```

the correlation value is equal to -0.7784268 ,
so we have a nagative relation between X & Y

### iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?

```{r}
#iv)
#make a f(x) function and calculate f(98):
f<-function(X){
  f=as.numeric(fit1$coefficients[1])+ as.numeric((X*fit1$coefficients[2]))
  return(f)
}

f(98)

#predictioin confidence interval 95% for f(98) predict:
predict(fit1,newdata = as.data.frame(x<-c(98)),interval = "confidence")

```

clearly we can see the lower and upper limit of our confidence interval and the prediction value.


## b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.

```{r}
#b)
x<-Auto$horsepower
y<-Auto$mpg
plot(x, y, col = "orange", type = "p")
abline(fit1,col="black")

```

## c) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

```{r}
#c)
par(mfrow= c(1,2))
plot(fit1,col="orange")

```

i think that we have three out lievs points and there is no high leverge points.
and the QQplots show the normality distrubtion.

End.

-----------------------------------------------------------------------

# 4) This question involves the use of multiple linear regression on the Auto data set.

## a) Produce a scatterplot matrix which includes all of the variables in the data set.

```{r}
#Exercise forth(ninth of Book):
#a):
matplot(Auto[1:8],col = c("Red","Blue","Black","yellow","orange","Green","Brown",85),ylab = "Auto Data",type = "l")

```

## b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, cor() which is qualitative.

```{r}
#b)
(as.matrix(cor(Auto[1:8])))
```

## c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:

```{r}
#c)
y<-Auto$mpg
x1<-Auto$cylinders
x2<-Auto$displacement
x3<-Auto$horsepower
x4<-Auto$weight
x5<-Auto$acceleration
x6<-Auto$year
x7<-Auto$origin
fit1<-lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(fit1)
```

### i. Is there a relationship between the predictors and the response?

```{r}
#i)
plot(x1,y,col="orange")

plot(x2,y,col="orange")

plot(x3,y,col="orange")

plot(x4,y,col="orange")

plot(x5,y,col="orange")

plot(x6,y,col="orange")

plot(x7,y,col="orange")


summary(fit1)[8]
```

X2,X3,X4 have negative linear relationship with resonse.

X5 have posetvie relation with response.

### ii. Which predictors appear to have a statistically significant relationship to the response?

```{r}
#ii)
summary(fit1)[4]
anova(fit1)
```

all of the predictors have statistically significant relationship to the response Except X5.

### iii. What does the coefficient for the year variable suggest?

```{r}
#iii)
coefficients(fit1)[7]
```

## d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit.Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r}
#d)
par(mfrow= c(1,2))
plot(fit1,col="Orange")

e<-residuals(fit1)
s<-rstandard(fit1)
t<-rstudent(fit1)
par(mfrow=c(1,3))
plot(x1,e,col="Orange")
abline(h=0,col="black")
plot(x1,s,col="Orange")
abline(h=0,col="black")
plot(x1,t,col="Orange")
abline(h=0,col="black")
plot(x2,e,col="Orange")
abline(h=0,col="black")
plot(x2,s,col="Orange")
abline(h=0,col="black")
plot(x2,t,col="Orange")
abline(h=0,col="black")
plot(x3,e,col="Orange")
abline(h=0,col="black")
plot(x3,s,col="Orange")
abline(h=0,col="black")
plot(x3,t,col="Orange")
abline(h=0,col="black")
plot(x4,e,col="Orange")
abline(h=0,col="black")
plot(x4,s,col="Orange")
abline(h=0,col="black")
plot(x4,t,col="Orange")
abline(h=0,col="black")
plot(x5,e,col="Orange")
abline(h=0,col="black")
plot(x5,s,col="Orange")
abline(h=0,col="black")
plot(x5,t,col="Orange")
abline(h=0,col="black")
plot(x6,e,col="Orange")
abline(h=0,col="black")
plot(x6,s,col="Orange")
abline(h=0,col="black")
plot(x6,t,col="Orange")
abline(h=0,col="black")
plot(x7,e,col="Orange")
abline(h=0,col="black")
plot(x7,s,col="Orange")
abline(h=0,col="black")
plot(x7,t,col="Orange")
abline(h=0,col="black")
```

at the top of the QQplot we can see some unusually points that are not in y=x linear.

we can see that we dont have fixed standard devation and i think that its posetive function of our predictors.

and we can see some outlievs and High leverage points in our model.

## e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?

```{r}
#e)
full.fit<-lm(y~x1+x2+x3+x4+x5+x6+x7+x1*x2+x1*x3+x1*x4+x1*x5+x1*x6+x1*x7+x2*x3+x2*x4+x3*x5+x3*x6+x3*x7+x4*x5+x4*x6+x4*x7+x5*x6+x5*x7+x6*x7)
summary(full.fit)
anova(full.fit)
```

now we can see that or p_value for predictors are signifact when is lower than 0.05.

the bethaj for j=0,1,2,3,4,5,6,7 is equal to the Pr(>|t|) column.

the Residual standard error = 2.714, R-squared = 0.8791.

## f) Try a few different transformations of the variables, such as log(X), âˆšX, X2. Comment on your findings.
```{r}
#f)
#the log(X) transformation:
y<-Auto$mpg
x1<-log(Auto$cylinders)
x2<-log(Auto$displacement)
x3<-log(Auto$horsepower)
x4<-log(Auto$weight)
x5<-log(Auto$acceleration)
x6<-log(Auto$year)
x7<-log(Auto$origin)
fit4<-lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(fit4)
```

now we can see that or p_value for predictors are signifact when is lower than 0.05.

the bethaj for j=0,1,2,3,4,5,6,7 is equal to the Pr(>|t|) column.

the Residual standard error =3.069 , R-squared =0.8482 .

```{r}
#the second root of X transformation:
y<-Auto$mpg
x1<-sqrt(Auto$cylinders)
x2<-sqrt(Auto$displacement)
x3<-sqrt(Auto$horsepower)
x4<-sqrt(Auto$weight)
x5<-sqrt(Auto$acceleration)
x6<-sqrt(Auto$year)
x7<-sqrt(Auto$origin)
fit5<-lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(fit5)
```

now we can see that or p_value for predictors are signifact when is lower than 0.05.

the bethaj for j=0,1,2,3,4,5,6,7 is equal to the Pr(>|t|) column.

the Residual standard error = 3.21, R-squared = 0.8338.

```{r}
#the X power to two transformation:
y<-Auto$mpg
x1<-(Auto$cylinders)^2
x2<-(Auto$displacement)^2
x3<-(Auto$horsepower)^2
x4<-(Auto$weight)^2
x5<-(Auto$acceleration)^2
x6<-(Auto$year)^2
x7<-(Auto$origin)^2
fit6<-lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(fit6)
```

now we can see that or p_value for predictors are signifact when is lower than 0.05.

the bethaj for j=0,1,2,3,4,5,6,7 is equal to the Pr(>|t|) column.

the Residual standard error = 3.539, R-squared = 0.7981.

End.

-----------------------------------------------------------------------

# 5) This question should be answered using the Carseats data set.
## a) Fit a multiple regression model to predict Sales using Price, Urban, and US.

```{r}
#Exercise fivth(Ten of Book):
#introducing the variables:for classifactions qualitative variables(Yes=2    , No=1)

#a)
y<-Carseats$Sales
x1<-Carseats$Price
x2<-as.integer(Carseats$Urban)
x3<-as.integer(Carseats$US)
fit1<-lm(y~x1+x2+x3)
```

yes =2  and no =1

## b) Provide an interpretation of each coefficient in the model. Be carefulâ€”some of the variables in the model are qualitative!

```{r}
#b)
summary(fit1)
```

just X3 have posetive relationship with response.


## c) Write out the model in equation form, being careful to handle the qualitative variables properly.

```{r}
#c)
f<-function(X1,X2,X3){
  f<-(as.numeric(coef(fit1)[1])+ (as.numeric(coef(fit1)[2])*X1)+ (as.numeric(coef(fit1)[3])*X2)+ (as.numeric(coef(fit1)[4])*X3))
  return(f)
}
```

## d) For which of the predictors can you reject the null hypothesis H0 : Î²j = 0?

```{r}
#d)
anova(fit1)
anova(fit1)[5]
```

now we can see that all of our predictors are signifact Except X2.

## e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

```{r}
#e)
Reduce.fit<-lm(Sales~US+Price,data = Carseats)

```

## f) How well do the models in (a) and (e) fit the data?

```{r}
#f)
summary(Reduce.fit)
```

the R squared is fixed(0.2393) when we remove the Urban perdictors form our model.

the residuals standard error just is lower(0.003) than the full model.

## g) Using the model from (e), obtain 95 % confidence intervals for the coefficient(s).

```{r}
#g)
confint(fit1,level = 0.95)
```

## h) Is there evidence of outliers or high leverage observations in the model from (e)?

```{r}
#h)
par(mfrow=c(1,2))
plot(fit1,col="orange")
```

i think that our residuals have standard normal distrubtion.

at the first devation is low then more after that again lower. 

we have some lievs andHigh leverage points.

End.

-----------------------------------------------------------------------
